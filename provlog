#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Tool to create and manage sidecar provenance record for some target file.

Usage:
  prvlog [options] PRVFILE
  prvlog [-v] [-H] (-c|-u) PRVFILE
  prvlog [-v] [-H] -M=TARGURI PRVFILE
  prvlog [-v] [-x CHECKSUM] [-l] [-m MSG] -o PRVFILE -t TARGFILE
  prvlog [-v] [-r] -m MSG -T TARGDIR
  prvlog [-v] [-H] [-a] [-r] (-s TARGDIR | -S TARGDIR)
  prvlog -h 
  prvlog -V 

Options:
    -h            Display this help information
    -H            Check hostnames when verifying fingerprints
    -a            Test all provlog files found (default = only stale files)
    -d            Display debugging info
    -c            Check that PRVFILE is up-to-date and valid
    -t TARGFILE   Create new sidecar log for target TARGFILE
    -T TARGDIR    Traverse hierarchy below TARGDIR and create new sidecar for every file found
    -x CHECKSUM   Verify target still matches existing MD5 value computed earlier
    -m "LOG STR"  Use "LOG STR" for log entry instead of prompting for interactive input. 
    -M TARGURI    Target file referenced in PRVFILE has been moved to TARGURI. Implies -u.
    -o PRVFILE    Output sidecar to PRVFILE
    -r            Report what would happen only. Don't perform actual operation.
    -u            Update PRVFILE, adding new checksum and log entry if it has changed
    -l            Prompt user for a new log msg to add to the record
    -s TARGDIR    Scan a directory and check all *.prvlog files found
    -S TARGDIR    Same as -s, but scans directories recursively
    -v            Display verbose output
    -V            Print version information

TARGFILE:
    FILE:machinename:///path/to/file/on/that/machine
    POSTGRESQL:machinename:schema.tablename
"""
import os        # for accessing local file system
import sys       # for printing to stderr for errors
import hashlib   # for generating MD5 hashes
import socket    # for accessing hostname of current machine
import datetime  # for adding date stamp to log records 
import subprocess as sp # for running shell processes
from docopt import docopt # for processing command line args

"""
28431f3f8ae3397d957f04150b1eff91  hostname://path/to/file/VIC_W2_Eth_ToP_5min_20210115.zip
@ 2021-01-20 11:22:17 Original file creation. 
< 2021-01-20 11:22:17 28431f3f8ae3397d957f04150b1eff91  hostname://path/to/file/VIC_W2_Eth_ToP_5min_20210115.zip
^80e0d410f20b970439a2e8133de7781f

Structure of a .PRVLOG file:
    Top line is always: MD5SUM  FNAME
        Represents most recent checksum of the subject file
        This top line was intentionally designed to reflect the
        common format of checksum files, so that pre-existing 
        checksums can be grandfathered into the PRV chain. 

        But a runtime warning will be issued by the prv system
        if that line does not contain a machine name and ://
        prefix. This is to give the operator a chance to update that
        entry with more complete information. In practice, however, this
        information is sometimes not available, so if it is not
        remediated, the historical information will be transfered as is to the
        comment section and the new top line will contain proper machine
        and prefix syntax. (Or possibly record it with the key value UNKNOWN for 
        machine name.)
                
    Followed by any number of comment entries.

    Comment consists of an @ line and a < line.

    @ Is a timestamped comment regarding a provenance/integrity change.

    < Is a timestamped snapshot of the checksum and file path taken from 
      top line of this file prior to adding this comment.

    Last line of the file is the ^ line, which is just the checksum of all the lines of this file, prior to this line.

    Any operation on this file will fail with a warning message
    if the ^ line does not match the previous content.

So every distinct variant in the lineage of FILE is documented in
the PRV file, including its checksum value and its fully
normalized storage path and name. (Which will help us locate
backups of that file on those previous machines, if nec.)

In addition, this file also serves as its own checksum record.

PRV Scanning:
    On a PRV-enabled computer, a service runs on a regular schedule,
    searching for .prv files and verifying that the active checksum
    value for its subject file is still valid, and that the
    self-checksum at the end is also still valid.

    If the subject file checksum is ever found to be incorrect, the
    operator is notified. 

    If the mismatch is caused by an accidental alteration, the file
    can be restored and no changes are required to the PRV file. But
    if the alteration was intentional, then the operator issues
    another prv change, which updates the PRV file, along with an
    explanation of the change.

    For data stored in non-atomic form, such as tables in a DB, there
    is no simple way to run a PRV-scan. Instead, a regularly scheduled 
    pre-prv script can be run to generate a DB-fingerprint file of the 
    table, which can then be tracked within the PRV system.

    (Alternatively, the table could be intuited from the path and
recomputed at verification time, but that would require the PRV
system to have an extensible scripting system to let it probe
paths on different DBs and protocols. It's simpler to leave that
to the operator to set up in an independent cron job.)


For files that can be accommodated within git, some of these
features are redundant. After all, git can tell you when a file
has been changed. Unfortunately, git does not play nicely with
enormous datasets, cannot provide much information about the
nature of a change to binary content, and does not have access to
provenance history before it was first moved into git control on 
this machine.

prv log PRVFILE
prv log -t CHKSUMFILE PRVFILE
prv verify PRVFILE
"""

verbose = False
trace = False

def debug(outstr):
    global trace
    if trace:
        print(outstr)

def mention(outstr):
    global verbose, trace
    if verbose or trace:
        print(outstr)

def notify(outstr):
    print(outstr, file=sys.stderr)

def verify_prvlog_contents(filepath):
    """
    Given a filehandle to a prvlog file, parse the contents and verify its fingerprint.
    Returns True if file is validated.
    Prints warning and returns False if file is not validated.
    """
    with open(filepath, 'r') as fh:
        content = ''
        for line in fh.readlines():
            if line[0] == '^': # is this the validation fingerprint?
                saved_fingerprint = line[1:].strip()
                computed_fingerprint = hashlib.md5(content.encode('utf-8')).hexdigest()
                debug(f"Saved: {saved_fingerprint}")
                debug(f"Calcd: {computed_fingerprint}")
                if saved_fingerprint == computed_fingerprint:
                    return True
                else:
                    notify("Log file content not well-formed.  Internal checksums mismatched.")
                    return False
            content += line
        debug(f"No integrity fingerprint found. File '{filepath}' is not valid.")
        return False


def verify_target_file(filepath, expected_fingerprint):
    """
    Given a filehandle to a target file and a known fingerprint,
    compute a fingerprint for the content of the target and ensure  
    that it matches the expected.
    Returns True if file is validated.
    Returns False if file is not validated.
    """
    mention(f"Verifying fingerprint for path {filepath} is {expected_fingerprint}")
    uri, calculated_fingerprint = fingerprint_local_file(filepath)
    foo = calculated_fingerprint and (calculated_fingerprint == expected_fingerprint)
    mention(f"Verification is {foo} because {calculated_fingerprint} vs {expected_fingerprint}")
    return foo


def verify_target_dir(dirpath, expected_fingerprint):
    """
    Given a path to a target dir and a known fingerprint,
    compute a fingerprint for the content of the target and ensure  
    that it matches the expected.
    Returns True if file is validated.
    Returns False if file is not validated.
    """
    mention(f"Verifying that fingerprint for path {dirpath} == {expected_fingerprint}")
    uri, calculated_fingerprint = fingerprint_local_dir(dirpath)
    foo = calculated_fingerprint and (calculated_fingerprint == expected_fingerprint)
    mention(f"Verification is {foo} because {calculated_fingerprint} vs {expected_fingerprint}")
    return foo


def verify_target_pgtable(tablepath, expected_fingerprint):
    """
    Given a schema.table path to a target table and a known fingerprint,
    compute a fingerprint for the content of the table and ensure  
    that it matches the expected.
    Returns True if table is validated.
    Returns False if table is not validated.
    """
    mention(f"Verifying fingerprint for path {tablepath} is {expected_fingerprint}")
    uri, calculated_fingerprint = fingerprint_postgresql_table(tablepath)
    foo = calculated_fingerprint and (calculated_fingerprint == expected_fingerprint)
    mention(f"Verification is {foo} because {calculated_fingerprint} vs {expected_fingerprint}")
    return foo


def fingerprint_local_file(targfile):
    """
    Given a path to a file, compute the MD5 checksum of that
    target, and do so in a manner that is efficient for large
    files. Returns the hexdigest string of the computed checksum.
    """
    blk_size_to_read = 65536
    filehash = hashlib.md5()
    with open(targfile, 'rb') as f:
        while (True):
            read_data = f.read(blk_size_to_read)
            if not read_data:
                break
            filehash.update(read_data)
        fing = filehash.hexdigest()
        current_machine = socket.gethostname()
        absuri = f"FILE:{current_machine}://{os.path.abspath(targfile)}"
        return absuri, fing
    return None, None


def fingerprint_local_dir(targetpath):
    """
    Given a path to a directory, compute the MD5 checksum of the
    file and directory names in that folder, in sorted order,
    without recursing into subfolders. 
    Returns the hexdigest string of the computed checksum.
    """
    if not os.path.isdir(targetpath):
        return None, None

    # fingerprint of a directory is a fingerpint on the sorted
    # list of files within that dir, but excludes all prvlog
    # files. (Because we're only concerned with monitoring
    # changes to the payload in the dir, not the other monitoring
    # files.)
    filelist = [x for x in os.listdir(targetpath) if not x.endswith('.prvlog')]
    listing = '\n'.join(sorted(filelist))

    dirhash = hashlib.md5()
    dirhash.update(listing.encode())
    fing = dirhash.hexdigest()
    current_machine = socket.gethostname()
    absuri = f"DIR:{current_machine}://{os.path.abspath(targetpath)}"
    return absuri, fing


def fingerprint_postgresql_table(uri):
    """
    Given a URI to a postgres database table, connect to the 
    table and compute a fingerprint for the current contents.
    Note that PG does not support a table-wide MD5 mechanism, 
    so we compute something manually. In short, we convert each
    row into a text string, which is then checksummed. We then
    convert that checksum into a bigint and sum the bigints for
    all the rows of the table. This produces a reasonably
    non-colliding fingerprint that will be consistent for 
    a table, regardless of what order PG elects to process the 
    rows in.
    """
    mention(f"Parsing uri {uri}")
    [dbname,tablepath] = uri.split(':')
    sql = f"SELECT SUM(('x'||SUBSTR(MD5(f::TEXT),1,16))::BIT(64)::BIGINT) FROM {tablepath} AS f;"
    mention(f"SQL: {sql}")
    cmd = ['psql', dbname, '-c', sql]
    mention(f"CMD: {cmd}")
    result = sp.run(cmd, stdout=sp.PIPE, encoding='utf-8')
    lines = result.stdout.split('\n')
    fing = lines[2].strip()
    return f"POSTGRESQL:{uri}", fing


def save_prvlog_file(targetline, oldlogs, newlogs, prvfile):
    """
    Given the content elements and a filepath, save the content to the
    filepath and append a content checksum. Content elements are
    expected to be strings, with required markup symbols and carriage
    returns included.
    """
    content = '\n'.join([x.strip() for x in [targetline, oldlogs, newlogs] if x])
    content += '\n'
    computed_fingerprint = hashlib.md5(content.encode('utf-8')).hexdigest()
    with open(prvfile, 'w') as fh:
        fh.write(content)
        fh.write(f"^{computed_fingerprint}\n")


def prompt_for_log_entry(prompt=''):
    """
    Ask the user to provide a string describing the current
    version of the target file.  
    Returns the provided string in the form of a PRVFILE comment
    entry.
    """
    if not prompt:
        prompt = "Describe changes made to the target file: "
    desc = input(prompt.strip() + ' ')
    return desc.strip()


def unpack_log_file(fpath):
    """
    Given a path to a prvlog file, unpack it and return
    targetline, comments, validationline
    """
    lines = []
    with open(fpath, 'r') as fh:
        for line in fh.readlines():
            if line.strip():
                lines.append(line)

    # verify that content conforms at least somewhat to expected format
    # content should be non-empty
    if not lines:
        return None, None, None
    # content will have at least one line 
    # beginning with each of @, <, and ^
    firstchars = set([line[0] for line in lines])
    if not all(x in firstchars for x in '@^<'):
        return None, None, None

    targetline = ''
    content = ''
    validationline = ''
    if lines:
        targetline = lines[0]
        content = ''.join(lines[1:-1])
        validationline = lines[-1]
    mention(f"Verifying against fingerprint {validationline.strip()}")
    return (targetline, content, validationline)


def unpack_uri(targetpath):
    """
    Given a uri of the form MODE:machine://path, split it up and
    return machinename, abspath
    For backward compatibility, if the first element is not in
    block caps, treat it as the machine name and assume the mode
    is an implicit FILE.
    """
    debug(f"Unpacking path: {targetpath}")
    if not ':' in targetpath:
        mention(f"Malformed target path: {targetpath}")
        return None, None, None
    mode,dum,remainder = targetpath.partition(':')
    if all([c != c.lower() for c in mode]): #mode is block caps
        machine,dum,fileuri = remainder.partition(':')
    else:
        mode = "FILE"
        machine = mode
        fileuri = remainder
    mention(f"Parsed URI {targetpath} into {mode}, {machine}, and {fileuri}")
    if mode == "FILE": 
        if not fileuri.startswith('///'):
            mention(f"Malformed file URI, slashless: {fileuri}")
            return None, None, None
        abspath = fileuri[2:].strip() #remove trailing newline
    elif mode == "DIR": 
        if not fileuri.startswith('///'):
            mention(f"Malformed directory URI, slashless: {fileuri}")
            return None, None, None
        abspath = fileuri[2:].strip() #remove trailing newline
    elif mode == "POSTGRESQL":
        abspath = fileuri.strip() #remove trailing newline
    return mode,machine,abspath


def check_prvfile_matches_its_target(prvfile, compare_hostnames=True, check_fresh=True):
    """
    Given a prvfile path, read it and test whether it is up to
    date against its target file.
    By default, all files are tested, but if check_fresh is
    False, only test sidecar file if it is older than its target
    file. This allows us to skip the expensive checksum against
    really big files if nothing has changed since the sidecar was
    created/updated. For files being skipped, success is returned
    without doing the test.

    Returns an integer result code:
    0 = success
    1 = poorly formed target file path
    2 = target file does not exist
    3 = target file machine name does not match current machine
    4 = fingerprint in prvfile does not match fingerprint of target file
    5 = problem parsing prvfile
    """
    # read old content
    result = 0
    targetline,content,validationline = unpack_log_file(prvfile)
    if not targetline or not validationline:
        notify("FAILURE: File missing required elements.")
        return 5,None

    # extract fingerprint and target path from targetline
    [expected_fingerprint, targetpath] = targetline.split(maxsplit=1)

    # verify machine name
    mode,machine,abspath = unpack_uri(targetpath)
    if mode == "FILE":
        if not machine or not abspath:
            notify("FAILURE: Bad machine or path")
            result = 1
        elif not os.path.isfile(abspath):
            notify(f"FAILURE: Target path '{abspath}' doesn't exist or is not a file")
            result = 2
        elif compare_hostnames and (machine != current_machine):
            notify(f"FAILURE: Wrong machine ({machine} vs {current_machine})")
            result = 3
        elif not verify_target_file(abspath, expected_fingerprint):
            notify("FAILURE: Fingerprint mismatch")
            result = 4
        else:
            mention("SUCCESS")
    elif mode == "DIR":
        if not machine or not abspath:
            notify("FAILURE: Bad machine or path")
            result = 1
        elif not os.path.isdir(abspath):
            notify(f"FAILURE: Target path '{abspath}' doesn't exist or is not a directory")
            result = 2
        elif compare_hostnames and (machine != current_machine):
            notify(f"FAILURE: Wrong machine ({machine} vs {current_machine})")
            result = 3
        elif not verify_target_dir(abspath, expected_fingerprint):
            notify(f"FAILURE: Fingerprint mismatch for {abspath}")
            result = 4
        else:
            mention("SUCCESS")
    elif mode == "POSTGRESQL":
        mention(f"VERIFYING PG URI: {mode} - {machine} - {abspath}")
        tablepath = machine + ':' + abspath
        if not verify_target_pgtable(tablepath, expected_fingerprint):
            notify("FAILURE: Fingerprint mismatch")
            result = 4
    else:
        notify(f"Unrecognized URI mode {mode}")
    return result,targetpath


def assemble_target_line(targeturi):
    """
    Given a target expressed in URI form, create the
    associated targetline to put inside the prv file.
    """
    mode,machine,targfile = unpack_uri(targeturi)
    # compute new targetline
    if not mode:
        absuri, fingerprint = fingerprint_local_file(targeturi)
        uri = f"FILE:{current_machine}://{os.path.abspath(targeturi)}"
    elif mode == 'FILE':
        absuri, fingerprint = fingerprint_local_file(targfile)
        uri = f"FILE:{current_machine}://{os.path.abspath(targfile)}"
    elif mode == 'DIR':
        absuri, fingerprint = fingerprint_local_dir(targfile)
        uri = f"DIR:{current_machine}://{os.path.abspath(targfile)}"
    elif mode == 'POSTGRESQL':
        absuri, fingerprint = fingerprint_postgresql_table(machine + ':' + targfile)
        uri = f"POSTGRESQL:{machine}:{targfile}"
    return(f"{fingerprint}  {uri}")


def create_log_for_target_file(targfilepath, logmsg, report_only=True):
    """
    Given an absolute target file path and a log message, create a new .prvlog
    file for the target, using logmsg as the initial comment. If
    report_only, then do not actually create the file.

    Returns 0 on success.
    Returns 1 if target file does not exist or is not readable.
    Returns 2 if prvlog already exists.
    """
    if not os.path.isfile(targfilepath):
        notify(f"{targfilepath} is not a valid file. Skipping.")
        return 1

    prvfilepath = targfilepath + ".prvlog"
    if os.path.isfile(prvfilepath):
        notify(f"{prvfilepath} already exists. Skipping {targfilepath}.")
        return 2

    notify(f"Creating new log file: {prvfilepath}")

    if report_only:
        return 0

    # assemble the header row info
    fingerprinter = fingerprinters['FILE']
    absuri,fingerprint = fingerprinter(targfilepath)

    newtargetline = f"{fingerprint}  {absuri}"

    # assemble the log comment for this first version
    datestr = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # assemble the log file content
    newlog = f"@ {datestr} {logmsg}\n< {datestr} {newtargetline}"

    # save the updated file
    save_prvlog_file(newtargetline, '', newlog, prvfilepath)


def create_log_for_target_dir(targdirpath, logmsg, report_only=True):
    """
    Given an absolute path to a dir and a log message, create a new .prvlog
    file for the target dir, using logmsg as the initial comment. If
    report_only, then do not actually create the file.

    Returns 0 on success.
    Returns 1 if target dir does not exist or is not readable.
    Returns 2 if prvlog already exists.
    """
    if not os.path.isdir(targdirpath):
        notify(f"{targdirpath} is not a valid dir. Skipping.")
        return 1

    prvdirpath = targdirpath + ".prvlog"
    if os.path.isdir(prvdirpath):
        notify(f"{prvdirpath} already exists. Skipping {targdirpath}.")
        return 2

    notify(f"Creating new log file: {prvdirpath}")

    if report_only:
        return 0

    # assemble the header row info
    fingerprinter = fingerprinters['DIR']
    absuri,fingerprint = fingerprinter(targdirpath)

    newtargetline = f"{fingerprint}  {absuri}"

    # assemble the log comment for this first version
    datestr = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # assemble the log file content
    newlog = f"@ {datestr} {logmsg}\n< {datestr} {newtargetline}"

    # save the updated file
    save_prvlog_file(newtargetline, '', newlog, prvdirpath)

      
# target files stored on different platforms are fingerprinted in
# different ways, because not all storage systems have a
# consistent way to extract a checksum.
fingerprinters = {'FILE': fingerprint_local_file, 
                  'DIR': fingerprint_local_dir,
                  'POSTGRESQL': fingerprint_postgresql_table,}

if __name__ == "__main__":
    args = docopt(__doc__, version='0.1.1')
    # print(args)
    verbose = args['-v']
    trace = args['-d']
    compare_hostnames = True

    # grab the hostname comparison flag
    compare_hostnames = args['-H']

    # grab the flag for testing/ignoring stale files
    skip_fresh = not args['-a']

    # grab the flag for simulating/suppressing the actual tests
    report_only = args['-r']

    # grab the log msg if it was provided
    origin = args['-m']

    current_machine = socket.gethostname()

    # handle the directory scanning args first
    targdir = ''
    if args['-s']:
        targdir = args['-s']
        recurse = False
    if args['-S']:
        targdir = args['-S']
        recurse = True
    if targdir:
        if not os.path.isdir(targdir):
            notify(f"Directory '{targdir}' does not exist.  Aborting.")
            exit(6)
        maxresult = 0
        numconsidered = 0
        numfresh = 0
        staleskipped = 0
        successes = []
        failures = []
        for root,dirnames,fnames in os.walk(targdir):
            for fname in fnames:
                if not fname.endswith('.prvlog'):
                    continue
                absfilepath = os.path.abspath(os.path.join(root,fname))
                # Now examine the PRVLOG file we've found
                if os.path.isfile(absfilepath):
                    mention(f"Checking {absfilepath}")
                    numconsidered += 1
                    # first, test whether we should be skipping this file
                    if skip_fresh:
                        targetline,content,validationline = unpack_log_file(absfilepath)
                        mode,machine,targfile = unpack_uri(targetline.split(maxsplit=1)[1])
                        if mode == "POSTGRESQL":
                            numfresh += 1
                            # target is a DB table, which has no
                            # date to compare, so skip it.
                            continue
                        if not os.path.exists(targfile):
                            numfresh += 1
                            failures.append(absfilepath)
                            continue
                        sidecar_timestamp = os.path.getmtime(absfilepath)
                        target_timestamp = os.path.getmtime(targfile)
                        if target_timestamp < sidecar_timestamp:
                            numfresh += 1
                            # sidecar is newer than target so it's fresh. 
                            # Skip it.
                            continue
                    # now actually test it
                    if report_only: #unless we're suppressing the actual tests
                        staleskipped += 1
                    else:
                        result,targetpath = check_prvfile_matches_its_target(absfilepath, compare_hostnames)
                        maxresult = max(result, maxresult)
                        if result > 0:
                            failures.append(absfilepath)
                        else:
                            successes.append(absfilepath)
                else:
                    notify(f"Can't find {absfilepath}")
            if not recurse:
                break
    
        notify(f"{numconsidered} *.prvlog files found")
        if report_only:
            notify(f"{numfresh} were up-to-date")
            notify(f"{staleskipped} would have been tested but skipped by -r flag")
        elif failures:
            # mention(f"{len(successes)+len(failures)} *.prvlog files found.")
            # mention(f"{len(successes)} validated successfully.")
            # notify(f"{len(failures)} files failed to validate.")
            notify(f"{len(successes)+len(failures)} required testing")
            notify(f"{len(successes)} succeeded")
            notify(f"{len(failures)} failed")
            notify('\n'.join(['  '+x for x in failures]))
        else:
            # notify(f"{len(successes)+len(failures)} prvlog file(s) found and validated.")
            notify(f"{len(successes)+len(failures)} required testing")
            notify(f"{len(successes)} succeeded")
            if verbose:
                mention('\n'.join(['  '+x for x in successes]))
        exit(maxresult)


    # All other options refer to a specific PRVFILE, so 
    # verify that before proceeding.
    prvfile = args['PRVFILE']
    if prvfile: 
        if os.path.isfile(prvfile):
            if verify_prvlog_contents(prvfile):
                mention("Log file content is well-formed.")
            else:
                notify(f"Log file {os.path.abspath(prvfile)} is not well-formed.")
                exit(7)
        else:
            notify(f"Log file '{prvfile}' does not exist")

    if args['-T']:
        targdir = args['-T']
        if not targdir:
            notify("No target directory provided. Aborting.")
            exit(6)

        if not os.path.isdir(targdir):
            notify(f"Directory '{targdir}' does not exist.  Aborting.")
            exit(6)

        if not args['-m']:
            notify(f"-T option requires -m, but none found.  Aborting.")
            exit(9)
        else:
            logmsg = args['-m']

        maxresult = 0
        numconsidered = 0
        numfresh = 0
        staleskipped = 0
        successes = []
        failures = []
        for root,dirnames,fnames in os.walk(targdir):
            for fname in fnames:
                if fname.endswith('.prvlog'): #don't log the log files
                    continue
                abstargpath = os.path.abspath(os.path.join(root,fname))
                mention(f"{root} with {fname} joins to {abstargpath}")
                if os.path.isfile(abstargpath):
                    create_log_for_target_file(abstargpath, logmsg, report_only)
            for dirname in dirnames:
                abstargpath = os.path.abspath(os.path.join(root,dirname))
                mention(f"{root} with {fname} joins to {abstargpath}")
                if os.path.isdir(abstargpath):
                    create_log_for_target_dir(abstargpath, logmsg, report_only)


    if args['-t']: # Create prvlog file for new target
        prevmd5 = None
        if args['-x']:
            prevmd5 = args['-x'].strip()
        targfile = args['-t']
        if not prvfile:
            if args['-o']:
                prvfile = args['-o']
                mention(f"Outputting to filename '{prvfile}'")
        if os.path.isfile(prvfile):
            notify(f"Log file '{prvfile}' already exists. Cannot proceed.")
            exit()
        mention(f"Creating new log file in {prvfile}.")

        # determine what kind of target is being fingerprinted
        if ':' in targfile:
            targtype,colon,targpath = targfile.partition(':')
            if not targtype in fingerprinters:
                notify(f"Don't know how to fingerprint files of type {targtype}")
                exit()
        else: 
            notify("No target type specified. Use format: FILE:filepath or POSTGRESQL:schema.tablename")
            exit()

        #prvfile = targpath + ".prvlog" # the version without the FILE: or POSTGRES: prefix

        # assemble the header row info
        fingerprinter = fingerprinters[targtype]
        absuri,fingerprint = fingerprinter(targpath)
        notify(f"Absolute URI of target: {absuri}")
        notify(f"Targfile {targfile} of type {targtype} at path {targpath} has fingerprint {fingerprint}")

        newtargetline = f"{fingerprint}  {absuri}"

        # assemble the log comment for this first version
        datestr = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        if not origin:
            origin = prompt_for_log_entry("Describe origin of the target file: ")

        # reassemble the file content
        newlog = f"@ {datestr} {origin}\n< {datestr} {newtargetline}"

        # if an existing md5 was provided and no longer matches
        # then the origin information belongs in old log data and
        # the explanation for the change becomes the newer log data
        if prevmd5 and prevmd5 != fingerprint:
            reason = prompt_for_log_entry("Target no longer matches provided MD5 checksum.\nDescribe change that has taken place: ")
            prevdate = prompt_for_log_entry("When was previous checksum created? [YYYY-MM-DD HH:MM:SS] ")
            prevtargetline = f"{prevmd5}  {uri}"
            # In this case, the origin statement should be
            # attached to the OLD checksum, and this new
            # explanation should be the current, active
            # comment, so shuffle the records
            origlog = f"@ {prevdate} {origin}\n< {prevdate} {prevtargetline}"
            newlog = f"@ {datestr} {reason}\n< {datestr} {newtargetline}"
            save_prvlog_file(newtargetline, origlog, newlog, prvfile)
        else:
            # save the updated file
            save_prvlog_file(newtargetline, '', newlog, prvfile)

        notify(f"Log file {prvfile} created.")
        exit()


    if args['-M']: # target file has been moved, so update the prvfile's target line
        oldtargetline,oldlogs,oldvalidationline = unpack_log_file(prvfile)
        newtargeturi = args['-M']
        newtargetline = assemble_target_line(newtargeturi)
        # now auto-create a log entry to record the path change
        datestr = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        msg = "Target file moved to new location"
        logline = f"@ {datestr} {msg}"
        logstamp = f"< {datestr} {newtargetline}"
        # assemble new content
        newlog = f"{logline}\n{logstamp}"
        save_prvlog_file(newtargetline, oldlogs, newlog, prvfile)

    if args['-u']: # Update PRVFILE, adding new MD5 checksum and comment if it has changed
        # read old content
        result,targetpath = check_prvfile_matches_its_target(prvfile, compare_hostnames)
        if result == 0:
            notify(f"Fingerprint still matches. No need to update.")
            exit()
        targetline,oldlogs,validationline = unpack_log_file(prvfile)
        mode, machine,targfile = unpack_uri(targetline.split()[1])
        # compute new targetline
        if mode == 'FILE':
            absuri, fingerprint = fingerprint_local_file(targfile)
            uri = f"FILE:{current_machine}://{os.path.abspath(targfile)}"
        elif mode == 'DIR':
            absuri, fingerprint = fingerprint_local_dir(targfile)
            uri = f"FILE:{current_machine}://{os.path.abspath(targfile)}"
        elif mode == 'POSTGRESQL':
            absuri, fingerprint = fingerprint_postgresql_table(machine + ':' + targfile)
            uri = f"POSTGRESQL:{machine}:{targfile}"
        targetline = f"{fingerprint}  {uri}"
        # query user for new comment
        datestr = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        if not origin:
            origin = prompt_for_log_entry("Log message to add: ")
        logline = f"@ {datestr} {origin}"
        logstamp = f"< {datestr} {targetline}"
        # assemble new content
        newlog = f"{logline}\n{logstamp}"
        # write new version of file
        save_prvlog_file(targetline,oldlogs,newlog, prvfile)
        notify("Log file updated.")
        exit()

    if args['-l']: # Log a new comment to the provenance chain in PRVFILE
        # First verify that the existing fingerprint is valid.
        # read and verify prvlog before altering it
        targetline,oldlogs,validationline = unpack_log_file(prvfile)
        [expected_fingerprint, targetpath] = targetline.split()
        machine,abspath = unpack_uri(targetpath)
        if not os.path.isfile(abspath) \
        or machine != current_machine \
        or not verify_target_file(abspath, expected_fingerprint):
            notify("Current fingerprint is not valid. Run with -c to get more information, or with -u to update the fingerprint.")
            exit(5)

        # fingerprint is valid, so continue
        # assemble the log comment to be added
        datestr = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        if not origin:
            origin = prompt_for_log_entry("Log message to add: ")
        logline = f"@ {datestr} {origin}"
        logstamp = f"< {datestr} {targetline}"
        # assemble new content
        newlog = f"{logline}\n{logstamp}"
        # save the updated file
        save_prvlog_file(targetline, oldlogs, newlog, prvfile)
        notify("Log message added.")
        exit()

    if args['-c']:
        result,targetpath = check_prvfile_matches_its_target(prvfile, compare_hostnames)
        if result == 0:
            notify(f"{prvfile} is up to date.")
        elif result == 1:
            notify(f"Target path '{targetpath}' is not well-formed.")
        elif result == 2:
            notify(f"Target file referenced in {prvfile} does not exist.")
            notify(f"Missing file: '{os.path.abspath(targetpath)}'")
        elif result == 3:
            notify(f"This is {current_machine} but {prvfile} was last updated on a different machine.")
            mention(f"Run prvlog with -u option to update log and explain migration history.")
        elif result == 4:
            notify(f"Target file '{targetpath}' does not match current fingerprint recorded in {prvfile}")
        exit(result)
